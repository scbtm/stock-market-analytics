{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Estimation Tutorial for Stock Market Analytics\n",
        "\n",
        "This notebook demonstrates how to use the estimation capabilities in the model factory for quantile regression and multi-target prediction.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The main estimation components are:\n",
        "1. **CatBoostMultiQuantileModel**: Scikit-learn compatible wrapper for multi-quantile regression\n",
        "2. **Estimation Functions**: Helper utilities for feature engineering and data preparation\n",
        "3. **Pipeline Integration**: Seamless integration with sklearn pipelines\n",
        "\n",
        "Key features:\n",
        "- **Multi-quantile prediction**: Simultaneous prediction of multiple quantiles\n",
        "- **Automatic categorical feature detection**: Handles mixed data types\n",
        "- **Monotonicity enforcement**: Ensures quantile ordering\n",
        "- **Pipeline compatibility**: Works with sklearn preprocessing and cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import estimation classes\n",
        "from stock_market_analytics.modeling.model_factory.estimation.estimators import (\n",
        "    CatBoostMultiQuantileModel\n",
        ")\n",
        "\n",
        "# Import estimation helper functions\n",
        "from stock_market_analytics.modeling.model_factory.estimation.estimation_functions import (\n",
        "    detect_categorical_features,\n",
        "    create_catboost_pool\n",
        ")\n",
        "\n",
        "# For data preprocessing and evaluation\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# For creating synthetic financial data\n",
        "import scipy.stats as stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Creating Realistic Financial Dataset\n",
        "\n",
        "We'll create a synthetic dataset that mimics real financial data with multiple asset types, market regimes, and mixed feature types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create synthetic financial dataset\n",
        "n_samples = 5000\n",
        "n_symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN', 'JPM', 'BAC', 'XOM', 'KO', 'PFE']\n",
        "sectors = ['Technology', 'Technology', 'Technology', 'Technology', 'Technology', \n",
        "          'Financial', 'Financial', 'Energy', 'Consumer', 'Healthcare']\n",
        "market_caps = ['Large', 'Large', 'Large', 'Large', 'Large', 'Large', 'Large', 'Large', 'Large', 'Large']\n",
        "\n",
        "# Generate time series data\n",
        "start_date = pd.Timestamp('2020-01-01')\n",
        "end_date = pd.Timestamp('2023-12-31')\n",
        "date_range = pd.date_range(start_date, end_date, freq='D')\n",
        "\n",
        "data_list = []\n",
        "\n",
        "for i in range(n_samples):\n",
        "    # Random date and symbol\n",
        "    date = np.random.choice(date_range)\n",
        "    symbol_idx = np.random.randint(len(n_symbols))\n",
        "    symbol = n_symbols[symbol_idx]\n",
        "    sector = sectors[symbol_idx]\n",
        "    market_cap = market_caps[symbol_idx]\n",
        "    \n",
        "    # Create features with realistic financial relationships\n",
        "    # Price-based features (normalized)\n",
        "    returns_1d = np.random.normal(0.001, 0.02)  # Daily return\n",
        "    returns_5d = returns_1d + np.random.normal(0, 0.01)  # 5-day return\n",
        "    returns_20d = returns_5d + np.random.normal(0, 0.02)  # 20-day return\n",
        "    \n",
        "    # Volume features\n",
        "    volume_ratio = np.random.lognormal(0, 0.5)  # Volume vs average\n",
        "    volume_trend = np.random.normal(0, 0.3)  # Volume trend\n",
        "    \n",
        "    # Volatility features\n",
        "    volatility_5d = np.random.gamma(2, 0.01)  # 5-day volatility\n",
        "    volatility_20d = volatility_5d + np.random.gamma(1, 0.005)  # 20-day volatility\n",
        "    \n",
        "    # Market features\n",
        "    market_return = np.random.normal(0.0005, 0.015)  # Market return\n",
        "    sector_return = market_return + np.random.normal(0, 0.01)  # Sector return\n",
        "    \n",
        "    # Technical indicators\n",
        "    rsi = np.random.uniform(20, 80)  # RSI indicator\n",
        "    macd = np.random.normal(0, 0.001)  # MACD\n",
        "    \n",
        "    # Fundamental features (categorical)\n",
        "    earnings_surprise = np.random.choice(['Beat', 'Meet', 'Miss'], p=[0.4, 0.3, 0.3])\n",
        "    analyst_rating = np.random.choice(['Buy', 'Hold', 'Sell'], p=[0.5, 0.4, 0.1])\n",
        "    \n",
        "    # Create target: next day return with realistic dependencies\n",
        "    # Base signal from momentum and mean reversion\n",
        "    momentum_signal = 0.1 * returns_1d + 0.05 * returns_5d\n",
        "    mean_reversion = -0.3 * returns_20d\n",
        "    \n",
        "    # Volatility effect\n",
        "    vol_effect = np.random.normal(0, volatility_5d)\n",
        "    \n",
        "    # Sector and market effects\n",
        "    market_effect = 0.5 * market_return\n",
        "    sector_effect = 0.3 * (sector_return - market_return)\n",
        "    \n",
        "    # Categorical effects\n",
        "    earnings_effect = {'Beat': 0.005, 'Meet': 0, 'Miss': -0.005}[earnings_surprise]\n",
        "    rating_effect = {'Buy': 0.002, 'Hold': 0, 'Sell': -0.002}[analyst_rating]\n",
        "    \n",
        "    # Combine all effects with noise\n",
        "    target = (\n",
        "        momentum_signal +\n",
        "        mean_reversion +\n",
        "        vol_effect +\n",
        "        market_effect +\n",
        "        sector_effect +\n",
        "        earnings_effect +\n",
        "        rating_effect +\n",
        "        np.random.normal(0, 0.01)  # Idiosyncratic noise\n",
        "    )\n",
        "    \n",
        "    data_list.append({\n",
        "        'date': date,\n",
        "        'symbol': symbol,\n",
        "        'sector': sector,\n",
        "        'market_cap': market_cap,\n",
        "        'returns_1d': returns_1d,\n",
        "        'returns_5d': returns_5d,\n",
        "        'returns_20d': returns_20d,\n",
        "        'volume_ratio': volume_ratio,\n",
        "        'volume_trend': volume_trend,\n",
        "        'volatility_5d': volatility_5d,\n",
        "        'volatility_20d': volatility_20d,\n",
        "        'market_return': market_return,\n",
        "        'sector_return': sector_return,\n",
        "        'rsi': rsi,\n",
        "        'macd': macd,\n",
        "        'earnings_surprise': earnings_surprise,\n",
        "        'analyst_rating': analyst_rating,\n",
        "        'target': target\n",
        "    })\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data_list)\n",
        "\n",
        "print(f\"Dataset created with {len(df)} samples\")\n",
        "print(f\"Features: {df.columns.tolist()[:-1]}\")\n",
        "print(f\"Target: {df.columns[-1]}\")\n",
        "print(f\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# Basic statistics\n",
        "print(f\"\\nTarget statistics:\")\n",
        "print(f\"  Mean: {df['target'].mean():.6f}\")\n",
        "print(f\"  Std: {df['target'].std():.6f}\")\n",
        "print(f\"  Min: {df['target'].min():.6f}\")\n",
        "print(f\"  Max: {df['target'].max():.6f}\")\n",
        "\n",
        "print(f\"\\nCategorical feature distributions:\")\n",
        "for col in ['sector', 'earnings_surprise', 'analyst_rating']:\n",
        "    print(f\"  {col}: {dict(df[col].value_counts())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Engineering and Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "feature_cols = [col for col in df.columns if col not in ['target', 'date']]\n",
        "X = df[feature_cols].copy()\n",
        "y = df['target'].copy()\n",
        "\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Target vector shape: {y.shape}\")\n",
        "\n",
        "# Detect categorical features automatically\n",
        "categorical_features = detect_categorical_features(X)\n",
        "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "print(f\"\\nAutomatic feature detection:\")\n",
        "print(f\"  Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
        "print(f\"  Numerical features ({len(numerical_features)}): {numerical_features}\")\n",
        "\n",
        "# Add some engineered features\n",
        "X['returns_momentum'] = X['returns_1d'] * X['returns_5d']  # Momentum interaction\n",
        "X['vol_adjusted_return'] = X['returns_1d'] / (X['volatility_5d'] + 1e-8)  # Risk-adjusted return\n",
        "X['relative_volume'] = np.log1p(X['volume_ratio'])  # Log-transformed volume\n",
        "X['market_sector_spread'] = X['sector_return'] - X['market_return']  # Sector alpha\n",
        "\n",
        "print(f\"\\nAfter feature engineering:\")\n",
        "print(f\"  Total features: {X.shape[1]}\")\n",
        "print(f\"  New features: returns_momentum, vol_adjusted_return, relative_volume, market_sector_spread\")\n",
        "\n",
        "# Update numerical features list\n",
        "numerical_features.extend(['returns_momentum', 'vol_adjusted_return', 'relative_volume', 'market_sector_spread'])\n",
        "\n",
        "# Split data for training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=X['sector']\n",
        ")\n",
        "\n",
        "print(f\"\\nData split:\")\n",
        "print(f\"  Training: {X_train.shape[0]} samples\")\n",
        "print(f\"  Testing: {X_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Basic Multi-Quantile Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define quantiles to predict\n",
        "quantiles = [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]\n",
        "\n",
        "# Initialize CatBoost multi-quantile model\n",
        "model = CatBoostMultiQuantileModel(\n",
        "    quantiles=quantiles,\n",
        "    random_state=42,\n",
        "    verbose=False,  # Set to True to see training progress\n",
        "    # CatBoost-specific parameters\n",
        "    iterations=500,\n",
        "    learning_rate=0.1,\n",
        "    depth=6,\n",
        "    l2_leaf_reg=3,\n",
        "    early_stopping_rounds=50\n",
        ")\n",
        "\n",
        "print(f\"Model initialized with {len(quantiles)} quantiles: {quantiles}\")\n",
        "print(f\"Categorical features will be auto-detected: {categorical_features}\")\n",
        "\n",
        "# Train the model\n",
        "print(\"\\nTraining multi-quantile model...\")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Model training completed!\")\n",
        "print(f\"  Best iteration: {model.best_iteration_}\")\n",
        "print(f\"  Feature importances available: {hasattr(model, 'feature_importances_')}\")\n",
        "\n",
        "# Make predictions\n",
        "print(\"\\nGenerating predictions...\")\n",
        "y_pred_quantiles_train = model.predict(X_train)\n",
        "y_pred_quantiles_test = model.predict(X_test)\n",
        "\n",
        "print(f\"Training predictions shape: {y_pred_quantiles_train.shape}\")\n",
        "print(f\"Test predictions shape: {y_pred_quantiles_test.shape}\")\n",
        "\n",
        "# Check prediction monotonicity\n",
        "def check_monotonicity(predictions):\n",
        "    \"\"\"Check if quantile predictions are monotonic.\"\"\"\n",
        "    violations = 0\n",
        "    for i in range(predictions.shape[0]):\n",
        "        if not np.all(np.diff(predictions[i, :]) >= 0):\n",
        "            violations += 1\n",
        "    return violations, violations / predictions.shape[0]\n",
        "\n",
        "train_violations, train_violation_rate = check_monotonicity(y_pred_quantiles_train)\n",
        "test_violations, test_violation_rate = check_monotonicity(y_pred_quantiles_test)\n",
        "\n",
        "print(f\"\\nMonotonicity check:\")\n",
        "print(f\"  Training violations: {train_violations} ({train_violation_rate:.3%})\")\n",
        "print(f\"  Test violations: {test_violations} ({test_violation_rate:.3%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate coverage for each quantile\n",
        "def calculate_coverage(y_true, y_pred_quantiles, quantiles):\n",
        "    \"\"\"Calculate empirical coverage for each quantile.\"\"\"\n",
        "    coverage = []\n",
        "    for i, q in enumerate(quantiles):\n",
        "        empirical_coverage = np.mean(y_true <= y_pred_quantiles[:, i])\n",
        "        coverage.append(empirical_coverage)\n",
        "    return np.array(coverage)\n",
        "\n",
        "# Calculate pinball loss for each quantile\n",
        "def pinball_loss(y_true, y_pred, quantile):\n",
        "    \"\"\"Calculate pinball loss for a specific quantile.\"\"\"\n",
        "    error = y_true - y_pred\n",
        "    return np.mean(np.maximum(quantile * error, (quantile - 1) * error))\n",
        "\n",
        "# Evaluate model performance\n",
        "coverage_train = calculate_coverage(y_train, y_pred_quantiles_train, quantiles)\n",
        "coverage_test = calculate_coverage(y_test, y_pred_quantiles_test, quantiles)\n",
        "\n",
        "# Calculate pinball losses\n",
        "pinball_losses_train = []\n",
        "pinball_losses_test = []\n",
        "\n",
        "for i, q in enumerate(quantiles):\n",
        "    train_loss = pinball_loss(y_train, y_pred_quantiles_train[:, i], q)\n",
        "    test_loss = pinball_loss(y_test, y_pred_quantiles_test[:, i], q)\n",
        "    pinball_losses_train.append(train_loss)\n",
        "    pinball_losses_test.append(test_loss)\n",
        "\n",
        "pinball_losses_train = np.array(pinball_losses_train)\n",
        "pinball_losses_test = np.array(pinball_losses_test)\n",
        "\n",
        "# Performance summary\n",
        "print(\"Quantile Regression Performance Analysis:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Quantile':<10} {'Target':<8} {'Train Cov':<10} {'Test Cov':<10} {'Train PB':<10} {'Test PB':<10}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, q in enumerate(quantiles):\n",
        "    print(f\"{q:<10.2f} {q:<8.2f} {coverage_train[i]:<10.3f} {coverage_test[i]:<10.3f} \"\n",
        "          f\"{pinball_losses_train[i]:<10.6f} {pinball_losses_test[i]:<10.6f}\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Mean':<10} {'':<8} {np.mean(coverage_train):<10.3f} {np.mean(coverage_test):<10.3f} \"\n",
        "      f\"{np.mean(pinball_losses_train):<10.6f} {np.mean(pinball_losses_test):<10.6f}\")\n",
        "\n",
        "# Coverage errors\n",
        "coverage_errors_train = np.abs(coverage_train - quantiles)\n",
        "coverage_errors_test = np.abs(coverage_test - quantiles)\n",
        "\n",
        "print(f\"\\nCoverage Error Analysis:\")\n",
        "print(f\"  Training - Mean: {np.mean(coverage_errors_train):.4f}, Max: {np.max(coverage_errors_train):.4f}\")\n",
        "print(f\"  Test - Mean: {np.mean(coverage_errors_test):.4f}, Max: {np.max(coverage_errors_test):.4f}\")\n",
        "\n",
        "# Model scoring (using median quantile)\n",
        "median_idx = len(quantiles) // 2\n",
        "r2_train = model.score(X_train, y_train)\n",
        "r2_test = model.score(X_test, y_test)\n",
        "\n",
        "print(f\"\\nModel R² Score (median quantile):\")\n",
        "print(f\"  Training: {r2_train:.4f}\")\n",
        "print(f\"  Test: {r2_test:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualization of Quantile Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualizations\n",
        "fig = plt.figure(figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Coverage analysis\n",
        "ax1 = plt.subplot(2, 3, 1)\n",
        "x_pos = np.arange(len(quantiles))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax1.bar(x_pos - width/2, coverage_train, width, label='Training', alpha=0.7)\n",
        "bars2 = ax1.bar(x_pos + width/2, coverage_test, width, label='Test', alpha=0.7)\n",
        "ax1.plot(x_pos, quantiles, 'ro-', label='Target', linewidth=2, markersize=6)\n",
        "\n",
        "ax1.set_xlabel('Quantiles')\n",
        "ax1.set_ylabel('Empirical Coverage')\n",
        "ax1.set_title('Quantile Coverage Analysis')\n",
        "ax1.set_xticks(x_pos)\n",
        "ax1.set_xticklabels([f'{q:.2f}' for q in quantiles], rotation=45)\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Pinball loss comparison\n",
        "ax2 = plt.subplot(2, 3, 2)\n",
        "ax2.bar(x_pos - width/2, pinball_losses_train, width, label='Training', alpha=0.7)\n",
        "ax2.bar(x_pos + width/2, pinball_losses_test, width, label='Test', alpha=0.7)\n",
        "\n",
        "ax2.set_xlabel('Quantiles')\n",
        "ax2.set_ylabel('Pinball Loss')\n",
        "ax2.set_title('Pinball Loss by Quantile')\n",
        "ax2.set_xticks(x_pos)\n",
        "ax2.set_xticklabels([f'{q:.2f}' for q in quantiles], rotation=45)\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Feature importance\n",
        "ax3 = plt.subplot(2, 3, 3)\n",
        "feature_importance = model.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "\n",
        "# Sort by importance\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': feature_importance\n",
        "}).sort_values('importance', ascending=True)\n",
        "\n",
        "# Show top 10 features\n",
        "top_features = importance_df.tail(10)\n",
        "ax3.barh(range(len(top_features)), top_features['importance'], alpha=0.7)\n",
        "ax3.set_yticks(range(len(top_features)))\n",
        "ax3.set_yticklabels(top_features['feature'])\n",
        "ax3.set_xlabel('Feature Importance')\n",
        "ax3.set_title('Top 10 Feature Importances')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Prediction intervals visualization (sample)\n",
        "ax4 = plt.subplot(2, 3, 4)\n",
        "n_viz = 100\n",
        "viz_idx = np.random.choice(len(y_test), n_viz, replace=False)\n",
        "viz_idx = np.sort(viz_idx)\n",
        "\n",
        "y_test_viz = y_test.iloc[viz_idx]\n",
        "y_pred_viz = y_pred_quantiles_test[viz_idx, :]\n",
        "\n",
        "# Sort by median prediction for better visualization\n",
        "median_pred = y_pred_viz[:, median_idx]\n",
        "sort_idx = np.argsort(median_pred)\n",
        "\n",
        "x_viz = np.arange(len(sort_idx))\n",
        "\n",
        "# Plot prediction intervals (10%-90%, 25%-75%)\n",
        "q10_idx, q25_idx, q75_idx, q90_idx = 1, 2, 4, 5  # Indices for quantiles\n",
        "\n",
        "ax4.fill_between(x_viz, y_pred_viz[sort_idx, q10_idx], y_pred_viz[sort_idx, q90_idx], \n",
        "                alpha=0.2, color='lightblue', label='10%-90% Interval')\n",
        "ax4.fill_between(x_viz, y_pred_viz[sort_idx, q25_idx], y_pred_viz[sort_idx, q75_idx], \n",
        "                alpha=0.3, color='lightgreen', label='25%-75% Interval')\n",
        "ax4.plot(x_viz, y_pred_viz[sort_idx, median_idx], 'b-', label='Median Prediction', linewidth=2)\n",
        "ax4.scatter(x_viz, y_test_viz.iloc[sort_idx], color='red', s=20, alpha=0.7, label='True Values')\n",
        "\n",
        "ax4.set_xlabel('Sample Index (sorted)')\n",
        "ax4.set_ylabel('Target Value')\n",
        "ax4.set_title('Prediction Intervals Visualization')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 5: Residuals analysis\n",
        "ax5 = plt.subplot(2, 3, 5)\n",
        "median_pred_test = y_pred_quantiles_test[:, median_idx]\n",
        "residuals = y_test - median_pred_test\n",
        "\n",
        "ax5.scatter(median_pred_test, residuals, alpha=0.6, s=10)\n",
        "ax5.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
        "ax5.set_xlabel('Predicted Values (Median)')\n",
        "ax5.set_ylabel('Residuals')\n",
        "ax5.set_title('Residuals vs Predicted')\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 6: QQ plot for residuals\n",
        "ax6 = plt.subplot(2, 3, 6)\n",
        "stats.probplot(residuals, dist=\"norm\", plot=ax6)\n",
        "ax6.set_title('Q-Q Plot of Residuals')\n",
        "ax6.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Pipeline Integration and Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a preprocessing pipeline that works with mixed data types\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "# Note: CatBoost handles categorical features automatically, but let's show integration\n",
        "# For demonstration, we'll create a simple preprocessor\n",
        "preprocessor = make_column_transformer(\n",
        "    (StandardScaler(), numerical_features),\n",
        "    remainder='passthrough'  # Keep categorical features as-is for CatBoost\n",
        ")\n",
        "\n",
        "# Create pipeline with preprocessing and model\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', CatBoostMultiQuantileModel(\n",
        "        quantiles=[0.1, 0.25, 0.5, 0.75, 0.9],\n",
        "        random_state=42,\n",
        "        verbose=False,\n",
        "        iterations=300,\n",
        "        learning_rate=0.1\n",
        "    ))\n",
        "])\n",
        "\n",
        "print(\"Pipeline created with preprocessing and CatBoost model\")\n",
        "print(f\"Numerical features to be scaled: {len(numerical_features)}\")\n",
        "print(f\"Categorical features (passthrough): {len(categorical_features)}\")\n",
        "\n",
        "# Fit the pipeline\n",
        "print(\"\\nTraining pipeline...\")\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_pipeline = pipeline.predict(X_test)\n",
        "print(f\"Pipeline predictions shape: {y_pred_pipeline.shape}\")\n",
        "\n",
        "# Score the pipeline (uses median quantile by default)\n",
        "pipeline_score = pipeline.score(X_test, y_test)\n",
        "print(f\"Pipeline R² score: {pipeline_score:.4f}\")\n",
        "\n",
        "# Cross-validation with the pipeline\n",
        "print(\"\\nPerforming cross-validation...\")\n",
        "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=3, scoring='r2')\n",
        "print(f\"CV R² scores: {cv_scores}\")\n",
        "print(f\"CV mean ± std: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "\n",
        "# Extract feature importance from the pipeline\n",
        "model_from_pipeline = pipeline.named_steps['model']\n",
        "feature_importance_pipeline = model_from_pipeline.feature_importances_\n",
        "print(f\"\\nFeature importances extracted from pipeline: {len(feature_importance_pipeline)} features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'model__iterations': [200, 300, 500],\n",
        "    'model__learning_rate': [0.05, 0.1, 0.15],\n",
        "    'model__depth': [4, 6, 8],\n",
        "    'model__l2_leaf_reg': [1, 3, 5]\n",
        "}\n",
        "\n",
        "# Create a simpler pipeline for faster tuning\n",
        "tuning_pipeline = Pipeline([\n",
        "    ('model', CatBoostMultiQuantileModel(\n",
        "        quantiles=[0.1, 0.5, 0.9],  # Fewer quantiles for faster tuning\n",
        "        random_state=42,\n",
        "        verbose=False,\n",
        "        early_stopping_rounds=30\n",
        "    ))\n",
        "])\n",
        "\n",
        "print(\"Starting hyperparameter tuning...\")\n",
        "print(f\"Parameter grid: {len(param_grid)} parameters\")\n",
        "print(f\"Total combinations: {np.prod([len(v) for v in param_grid.values()])}\")\n",
        "\n",
        "# Perform grid search with reduced scope for demonstration\n",
        "# In practice, you might use RandomizedSearchCV for larger grids\n",
        "grid_search = GridSearchCV(\n",
        "    tuning_pipeline,\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='r2',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit on a subset for demonstration (full dataset would take longer)\n",
        "sample_size = min(2000, len(X_train))\n",
        "sample_idx = np.random.choice(len(X_train), sample_size, replace=False)\n",
        "X_train_sample = X_train.iloc[sample_idx]\n",
        "y_train_sample = y_train.iloc[sample_idx]\n",
        "\n",
        "print(f\"\\nTuning on sample of {sample_size} training examples...\")\n",
        "grid_search.fit(X_train_sample, y_train_sample)\n",
        "\n",
        "print(f\"\\nGrid search completed!\")\n",
        "print(f\"Best score: {grid_search.best_score_:.4f}\")\n",
        "print(f\"Best parameters:\")\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "# Evaluate best model on full test set\n",
        "best_model = grid_search.best_estimator_\n",
        "best_score_test = best_model.score(X_test, y_test)\n",
        "print(f\"\\nBest model test score: {best_score_test:.4f}\")\n",
        "\n",
        "# Compare with default parameters\n",
        "print(f\"\\nComparison:\")\n",
        "print(f\"  Default model test score: {pipeline_score:.4f}\")\n",
        "print(f\"  Tuned model test score: {best_score_test:.4f}\")\n",
        "print(f\"  Improvement: {((best_score_test - pipeline_score) / abs(pipeline_score) * 100):.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Advanced Analysis: Sector-Specific Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze model performance by sector\n",
        "X_test_with_sector = X_test.copy()\n",
        "y_pred_full = model.predict(X_test)  # Use the original full model\n",
        "\n",
        "# Get median predictions for sector analysis\n",
        "median_pred = y_pred_full[:, median_idx]\n",
        "\n",
        "# Calculate metrics by sector\n",
        "sectors = X_test_with_sector['sector'].unique()\n",
        "sector_metrics = {}\n",
        "\n",
        "for sector in sectors:\n",
        "    sector_mask = X_test_with_sector['sector'] == sector\n",
        "    if sector_mask.sum() > 10:  # Only analyze sectors with sufficient data\n",
        "        y_true_sector = y_test[sector_mask]\n",
        "        y_pred_sector = median_pred[sector_mask]\n",
        "        y_quantiles_sector = y_pred_full[sector_mask, :]\n",
        "        \n",
        "        # Calculate metrics\n",
        "        mse = mean_squared_error(y_true_sector, y_pred_sector)\n",
        "        mae = mean_absolute_error(y_true_sector, y_pred_sector)\n",
        "        \n",
        "        # Calculate coverage for 25%-75% interval\n",
        "        q25_idx, q75_idx = 2, 4\n",
        "        coverage_50 = np.mean(\n",
        "            (y_true_sector >= y_quantiles_sector[:, q25_idx]) & \n",
        "            (y_true_sector <= y_quantiles_sector[:, q75_idx])\n",
        "        )\n",
        "        \n",
        "        sector_metrics[sector] = {\n",
        "            'n_samples': sector_mask.sum(),\n",
        "            'mse': mse,\n",
        "            'mae': mae,\n",
        "            'rmse': np.sqrt(mse),\n",
        "            'coverage_50': coverage_50,\n",
        "            'mean_prediction': y_pred_sector.mean(),\n",
        "            'mean_actual': y_true_sector.mean()\n",
        "        }\n",
        "\n",
        "# Display sector analysis\n",
        "print(\"Sector-Specific Performance Analysis:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Sector':<12} {'N':<6} {'RMSE':<8} {'MAE':<8} {'Coverage':<8} {'Pred Mean':<10} {'Actual Mean':<10}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for sector, metrics in sector_metrics.items():\n",
        "    print(f\"{sector:<12} {metrics['n_samples']:<6} {metrics['rmse']:<8.5f} {metrics['mae']:<8.5f} \"\n",
        "          f\"{metrics['coverage_50']:<8.2%} {metrics['mean_prediction']:<10.5f} {metrics['mean_actual']:<10.5f}\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Visualize sector performance\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: RMSE by sector\n",
        "sectors_list = list(sector_metrics.keys())\n",
        "rmse_values = [sector_metrics[s]['rmse'] for s in sectors_list]\n",
        "\n",
        "axes[0, 0].bar(sectors_list, rmse_values, alpha=0.7)\n",
        "axes[0, 0].set_title('RMSE by Sector')\n",
        "axes[0, 0].set_ylabel('RMSE')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Coverage by sector\n",
        "coverage_values = [sector_metrics[s]['coverage_50'] for s in sectors_list]\n",
        "axes[0, 1].bar(sectors_list, coverage_values, alpha=0.7, color='green')\n",
        "axes[0, 1].axhline(y=0.5, color='red', linestyle='--', label='Target (50%)')\n",
        "axes[0, 1].set_title('50% Interval Coverage by Sector')\n",
        "axes[0, 1].set_ylabel('Coverage')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Prediction vs Actual means by sector\n",
        "pred_means = [sector_metrics[s]['mean_prediction'] for s in sectors_list]\n",
        "actual_means = [sector_metrics[s]['mean_actual'] for s in sectors_list]\n",
        "\n",
        "x_pos = np.arange(len(sectors_list))\n",
        "width = 0.35\n",
        "\n",
        "axes[1, 0].bar(x_pos - width/2, pred_means, width, label='Predicted', alpha=0.7)\n",
        "axes[1, 0].bar(x_pos + width/2, actual_means, width, label='Actual', alpha=0.7)\n",
        "axes[1, 0].set_title('Mean Values by Sector')\n",
        "axes[1, 0].set_ylabel('Mean Return')\n",
        "axes[1, 0].set_xticks(x_pos)\n",
        "axes[1, 0].set_xticklabels(sectors_list, rotation=45)\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Sample sizes by sector\n",
        "sample_sizes = [sector_metrics[s]['n_samples'] for s in sectors_list]\n",
        "axes[1, 1].bar(sectors_list, sample_sizes, alpha=0.7, color='orange')\n",
        "axes[1, 1].set_title('Sample Size by Sector')\n",
        "axes[1, 1].set_ylabel('Number of Samples')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Interpretation and Feature Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance analysis\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"Top 15 Most Important Features:\")\n",
        "print(\"=\" * 40)\n",
        "for i, row in feature_importance_df.head(15).iterrows():\n",
        "    print(f\"{row['feature']:<25} {row['importance']:<10.4f}\")\n",
        "\n",
        "# Analyze feature importance by category\n",
        "feature_categories = {\n",
        "    'Returns': ['returns_1d', 'returns_5d', 'returns_20d', 'returns_momentum'],\n",
        "    'Volume': ['volume_ratio', 'volume_trend', 'relative_volume'],\n",
        "    'Volatility': ['volatility_5d', 'volatility_20d', 'vol_adjusted_return'],\n",
        "    'Market': ['market_return', 'sector_return', 'market_sector_spread'],\n",
        "    'Technical': ['rsi', 'macd'],\n",
        "    'Categorical': ['symbol', 'sector', 'market_cap', 'earnings_surprise', 'analyst_rating']\n",
        "}\n",
        "\n",
        "category_importance = {}\n",
        "for category, features in feature_categories.items():\n",
        "    category_features = [f for f in features if f in feature_importance_df['feature'].values]\n",
        "    if category_features:\n",
        "        category_imp = feature_importance_df[feature_importance_df['feature'].isin(category_features)]['importance'].sum()\n",
        "        category_importance[category] = category_imp\n",
        "\n",
        "print(f\"\\nFeature Importance by Category:\")\n",
        "print(\"=\" * 35)\n",
        "for category, importance in sorted(category_importance.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{category:<15} {importance:<10.4f} ({importance/sum(category_importance.values())*100:.1f}%)\")\n",
        "\n",
        "# Create feature importance visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Top features\n",
        "top_features = feature_importance_df.head(12)\n",
        "axes[0].barh(range(len(top_features)), top_features['importance'], alpha=0.7)\n",
        "axes[0].set_yticks(range(len(top_features)))\n",
        "axes[0].set_yticklabels(top_features['feature'])\n",
        "axes[0].set_xlabel('Feature Importance')\n",
        "axes[0].set_title('Top 12 Feature Importances')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Category importance\n",
        "categories = list(category_importance.keys())\n",
        "importances = list(category_importance.values())\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, len(categories)))\n",
        "\n",
        "wedges, texts, autotexts = axes[1].pie(importances, labels=categories, autopct='%1.1f%%', \n",
        "                                      colors=colors, startangle=90)\n",
        "axes[1].set_title('Feature Importance by Category')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze predictions by categorical features\n",
        "print(f\"\\nPrediction Analysis by Categorical Features:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for cat_feature in ['sector', 'earnings_surprise', 'analyst_rating']:\n",
        "    if cat_feature in X_test.columns:\n",
        "        print(f\"\\n{cat_feature.upper()}:\")\n",
        "        for category in X_test[cat_feature].unique():\n",
        "            mask = X_test[cat_feature] == category\n",
        "            if mask.sum() > 5:  # Only show categories with sufficient samples\n",
        "                actual_mean = y_test[mask].mean()\n",
        "                pred_mean = median_pred[mask].mean()\n",
        "                n_samples = mask.sum()\n",
        "                print(f\"  {category:<15}: Actual={actual_mean:>8.5f}, Pred={pred_mean:>8.5f}, N={n_samples:>3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary and Best Practices\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **CatBoostMultiQuantileModel** provides a robust sklearn-compatible interface for multi-quantile regression\n",
        "2. **Automatic categorical feature detection** simplifies data preprocessing\n",
        "3. **Pipeline integration** enables seamless use with sklearn's preprocessing and cross-validation tools\n",
        "4. **Multi-quantile prediction** captures uncertainty and provides rich distributional information\n",
        "\n",
        "### Best Practices:\n",
        "\n",
        "1. **Feature Engineering**: Create domain-specific features (momentum, volatility-adjusted returns, etc.)\n",
        "2. **Categorical Features**: Let CatBoost handle categorical features automatically for optimal performance\n",
        "3. **Quantile Selection**: Choose quantiles based on your specific use case (risk management, portfolio optimization)\n",
        "4. **Cross-Validation**: Use proper CV strategies, especially for time series data\n",
        "5. **Hyperparameter Tuning**: Balance model complexity with overfitting risk\n",
        "6. **Domain Analysis**: Analyze performance by relevant segments (sectors, market conditions, etc.)\n",
        "\n",
        "### Model Deployment Considerations:\n",
        "\n",
        "1. **Monitoring**: Track prediction quality and coverage over time\n",
        "2. **Retraining**: Establish retraining schedules based on data drift detection\n",
        "3. **Feature Stability**: Monitor feature importance changes and distribution shifts\n",
        "4. **Performance Metrics**: Use appropriate metrics for quantile regression (pinball loss, coverage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final model summary\n",
        "print(\"FINAL MODEL ESTIMATION SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"Dataset: {len(df)} samples, {X.shape[1]} features\")\n",
        "print(f\"  Training: {len(X_train)} samples\")\n",
        "print(f\"  Testing: {len(X_test)} samples\")\n",
        "\n",
        "print(f\"\\nModel Configuration:\")\n",
        "print(f\"  Quantiles: {model.quantiles}\")\n",
        "print(f\"  Iterations: {model.best_iteration_}\")\n",
        "print(f\"  Categorical features: {len(categorical_features)}\")\n",
        "print(f\"  Numerical features: {len(numerical_features)}\")\n",
        "\n",
        "print(f\"\\nPerformance (R² Score):\")\n",
        "print(f\"  Training: {model.score(X_train, y_train):.4f}\")\n",
        "print(f\"  Test: {model.score(X_test, y_test):.4f}\")\n",
        "\n",
        "print(f\"\\nQuantile Coverage Analysis:\")\n",
        "print(f\"  Mean coverage error (train): {np.mean(coverage_errors_train):.4f}\")\n",
        "print(f\"  Mean coverage error (test): {np.mean(coverage_errors_test):.4f}\")\n",
        "print(f\"  Max coverage error (test): {np.max(coverage_errors_test):.4f}\")\n",
        "\n",
        "print(f\"\\nTop 3 Most Important Features:\")\n",
        "for i, row in feature_importance_df.head(3).iterrows():\n",
        "    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "print(f\"\\nMonotonicity Violations:\")\n",
        "print(f\"  Training: {train_violation_rate:.3%}\")\n",
        "print(f\"  Test: {test_violation_rate:.3%}\")\n",
        "\n",
        "print(\"\\n✅ Estimation tutorial completed successfully!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}